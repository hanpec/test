import torch

x = torch.tensor([1., -1.])
w = torch.tensor([1.0, 0.5], requires_grad=True)

loss = -torch.dot(x, w).sigmoid().log()
loss.backward()
print(loss.item())
print(w.grad)

'''
このプログラムは、PyTorchを使用して簡単な計算と勾配計算を行うものです。以下にプログラムの解説を示します。

import torch: PyTorchライブラリをインポートします。

x = torch.tensor([1., -1.]): ベクトルxを定義します。このベクトルは2つの要素（1.0と-1.0）を持っています。これは入力データです。
w = torch.tensor([1.0, 0.5], requires_grad=True): ベクトルwを定義します。このベクトルも2つの要素（1.0と0.5）を持っています。requires_grad=Trueは、PyTorchに対してこのテンソルの勾配（微分）を計算するよう指示しています。つまり、このテンソルに対して後で勾配計算が可能になります。

loss = -torch.dot(x, w).sigmoid().log(): 損失関数を計算します。この損失関数は、ベクトルxとベクトルwの内積を計算し、その結果にシグモイド関数を適用し、その結果に対数を取ったものです。このような損失関数は、通常、二値分類のロジスティック回帰などで使用されます。

loss.backward(): PyTorchの自動微分（autograd）機能を使用して、損失に対する勾配を計算します。これにより、変数wに関する勾配が計算され、w.gradに格納されます。

print(loss.item()): 計算された損失の値を表示します。.item()はテンソル内の単一のスカラー値を取得するために使用されます。
print(w.grad): 計算された勾配を表示します。これはwに関する偏微分値です。具体的には、w[0]とw[1]に関する偏微分値が表示されます。

このプログラムは、PyTorchの基本的なテンソル操作と自動微分機能をデモンストレーションするもので、機械学習モデルの訓練時に損失と勾配の計算がどのように行われるかを示しています。
'''